 ğŸ“š Books Web Scraper - Data Collection from E-commerce

 ğŸ¯ Contexte et Objectif

 ProblÃ©matique mÃ©tier
Dans l'industrie du e-commerce, la collecte automatisÃ©e de donnÃ©es produits est essentielle pour :
- Veille concurrentielle : Surveillance des prix et disponibilitÃ©s
- Analyse de marchÃ© : ComprÃ©hension de l'offre et positionnement
- Enrichissement de catalogues : Alimentation de bases de donnÃ©es produits

Ce projet dÃ©montre la capacitÃ© Ã  extraire, structurer et transformer des donnÃ©es web non structurÃ©es en datasets exploitables.

 Objectifs du projet
- DÃ©velopper un scraper web robuste pour extraire des donnÃ©es produits
- Collecter des informations structurÃ©es (titre, prix, disponibilitÃ©)
- Transformer les donnÃ©es HTML en format tabulaire (DataFrame)
- DÃ©montrer la maÃ®trise des fondamentaux de la collecte de donnÃ©es

 ğŸ—ï¸ Architecture technique

 Stack technique
- Langage : Python 3.x
- Web Scraping : BeautifulSoup4, Requests
- Data Processing : Pandas
- Target : Site e-commerce de dÃ©monstration (books.toscrape.com)

 Pipeline de collecte

```
1. HTTP Request
   â””â”€> GET request vers l'URL cible

2. HTML Parsing
   â””â”€> BeautifulSoup parse le contenu HTML

3. Data Extraction
   â”œâ”€> Identification des sÃ©lecteurs CSS
   â”œâ”€> Extraction des titres (attribut 'title')
   â”œâ”€> Extraction des prix (class 'price_color')
   â””â”€> Extraction des disponibilitÃ©s (class 'instock availability')

4. Data Structuring
   â””â”€> Construction de listes Python

5. Data Transformation
   â””â”€> Conversion en DataFrame Pandas

6. Output
   â””â”€> Dataset exploitable (CSV, Excel, base de donnÃ©es)
```

 Architecture du code

```
book_scraper.py
â”œâ”€â”€ Imports (requests, BeautifulSoup, pandas)
â”œâ”€â”€ Configuration (URL cible)
â”œâ”€â”€ HTTP Request & Response handling
â”œâ”€â”€ HTML Parsing (soup object)
â”œâ”€â”€ Data Extraction Loop
â”‚   â”œâ”€â”€ Title extraction
â”‚   â”œâ”€â”€ Price extraction
â”‚   â””â”€â”€ Availability extraction
â””â”€â”€ DataFrame Creation
```

 ğŸ“Š DonnÃ©es collectÃ©es

 Structure du dataset

| Colonne | Type | Description | Exemple |
|---------|------|-------------|---------|
| `Title` | String | Titre complet du livre | "A Light in the Attic" |
| `Price` | String | Prix avec devise | "Â£51.77" |
| `Availability` | String | Statut de disponibilitÃ© | "In stock" |

 VolumÃ©trie
- Page scrapÃ©e : 1 (page d'accueil)
- Produits par page : ~20 livres
- Taux de collecte : 100% (toutes les informations visibles)

 Exemple de donnÃ©es extraites

```
     Title                          Price    Availability
0    A Light in the Attic          Â£51.77   In stock
1    Tipping the Velvet            Â£53.74   In stock
2    Soumission                    Â£50.10   In stock
...
```

 ğŸ” Choix techniques

 Pourquoi BeautifulSoup et non Selenium ?

BeautifulSoup a Ã©tÃ© choisi car :
- âœ… SimplicitÃ© : Site statique sans JavaScript complexe
- âœ… Performance : Pas besoin de navigateur headless
- âœ… LÃ©gÃ¨retÃ© : Moins de dÃ©pendances et ressources
- âœ… RapiditÃ© : Parsing HTML direct vs rendu navigateur

Selenium serait nÃ©cessaire pour :
- Sites avec contenu chargÃ© dynamiquement (React, Vue.js)
- Interactions complexes (scroll infini, clics, formulaires)
- Sites nÃ©cessitant JavaScript pour afficher le contenu

 SÃ©lecteurs CSS utilisÃ©s

```python
# Article contenant un livre
'article.product_pod'

# Titre du livre (attribut HTML)
book.h3.a['title']

# Prix (class CSS)
'p.price_color'

# DisponibilitÃ© (class CSS composÃ©e)
'p.instock.availability'
```

Robustesse des sÃ©lecteurs :
- `class_='product_pod'` : SÃ©lecteur stable liÃ© Ã  la structure du site
- Utilisation d'attributs HTML natifs (`title`) pour Ã©viter les problÃ¨mes d'encodage
- `.text.strip()` pour nettoyer les espaces superflus

 ğŸš€ FonctionnalitÃ©s implÃ©mentÃ©es

 1. RequÃªte HTTP sÃ©curisÃ©e
```python
response = requests.get(url)
# Note : Ajout de gestion d'erreur recommandÃ© (voir AmÃ©liorations)
```

 2. Parsing HTML robuste
```python
soup = BeautifulSoup(response.content, 'html.parser')
# Parser 'html.parser' : plus rapide et sans dÃ©pendance externe
```

 3. Extraction multi-attributs
```python
for book in books:
    title = book.h3.a['title']           # Attribut HTML
    price = book.find('p', class_='price_color').text  # Texte d'Ã©lÃ©ment
    availability = book.find('p', class_='instock availability').text.strip()
```

 4. Structuration des donnÃ©es
```python
data = {
    'Title': titles,
    'Price': prices,
    'Availability': availabilities
}
df = pd.DataFrame(data)
```

 ğŸ“ˆ RÃ©sultats et MÃ©triques

 Performance
- Temps d'exÃ©cution : < 2 secondes pour une page
- Taux de succÃ¨s : 100% (site stable et prÃ©visible)
- DonnÃ©es extraites : 20 livres par exÃ©cution

 QualitÃ© des donnÃ©es
- ComplÃ©tude : 100% (tous les champs remplis)
- Format : DonnÃ©es brutes (nettoyage requis pour le prix Â£)
- CohÃ©rence : Structure identique pour tous les produits

 âš ï¸ Limites et AmÃ©liorations futures

 Limites actuelles

1. Scraping mono-page
   - Une seule page collectÃ©e (20 livres)
   - Site contient 50 pages (1000 livres total)
   
2. Absence de gestion d'erreurs
   ```python
   # ProblÃ¨mes potentiels non gÃ©rÃ©s :
   - Timeout de connexion
   - Erreur HTTP 404/500
   - Changement de structure HTML
   - Ã‰lÃ©ments manquants (livre sans prix)
   ```

3. DonnÃ©es non nettoyÃ©es
   - Prix en format string avec symbole Â£
   - DisponibilitÃ© avec texte superflu ("In stock")
   - Pas de conversion de types (price devrait Ãªtre float)

4. Pas de respect des bonnes pratiques
   - Absence de `robots.txt` check
   - Pas de rate limiting (risque de ban)
   - User-Agent non dÃ©fini
   - Pas de logs

5. Stockage temporaire
   - DonnÃ©es uniquement en mÃ©moire (DataFrame)
   - Pas de persistance (CSV, base de donnÃ©es)

 AmÃ©liorations proposÃ©es

 âœ… Court terme (1-2 jours)

1. Pagination complÃ¨te
```python
# Scraper toutes les pages (1-50)
for page in range(1, 51):
    url = f'https://books.toscrape.com/catalogue/page-{page}.html'
    # ... scraping logic
```

2. Gestion d'erreurs robuste
```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()  # LÃ¨ve exception si 4xx/5xx
except requests.exceptions.RequestException as e:
    print(f"Erreur de requÃªte : {e}")
```

3. Nettoyage des donnÃ©es
```python
# Conversion du prix en float
df['Price'] = df['Price'].str.replace('Â£', '').astype(float)

# Normalisation de la disponibilitÃ©
df['Availability'] = df['Availability'].apply(lambda x: 'In stock' if 'In stock' in x else 'Out of stock')
```

4. Export des donnÃ©es
```python
# Export CSV
df.to_csv('books_data.csv', index=False, encoding='utf-8')

# Export Excel (optionnel)
df.to_excel('books_data.xlsx', index=False)
```

 ğŸ”§ Moyen terme (1 semaine)

1. Configuration et bonnes pratiques
```python
# Headers pour simuler un navigateur
headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; BookBot/1.0)',
    'Accept': 'text/html,application/xhtml+xml'
}
response = requests.get(url, headers=headers)

# Rate limiting (respecter le site)
import time
time.sleep(1)  # 1 seconde entre chaque requÃªte
```

2. DonnÃ©es enrichies
```python
# Extraire des informations supplÃ©mentaires :
- Note du livre (rating stars)
- CatÃ©gorie
- Image de couverture (URL)
- Lien vers la page dÃ©taillÃ©e
```

3. Logging structurÃ©
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Scraping page {page}...")
logger.error(f"Ã‰chec extraction livre {title}")
```

4. Tests unitaires
```python
# Tester les fonctions d'extraction
def test_extract_title():
    sample_html = '<h3><a title="Test Book"></a></h3>'
    # ... assertions
```

 ğŸš€ Long terme (1 mois)

1. Architecture modulaire
```python
# SÃ©paration des responsabilitÃ©s
class BookScraper:
    def __init__(self, base_url):
        self.base_url = base_url
    
    def fetch_page(self, url):
        """RÃ©cupÃ¨re le contenu d'une page"""
        pass
    
    def parse_books(self, soup):
        """Extrait les livres d'une page"""
        pass
    
    def clean_data(self, df):
        """Nettoie et transforme les donnÃ©es"""
        pass
    
    def save_to_db(self, df):
        """Sauvegarde en base de donnÃ©es"""
        pass
```

2. Base de donnÃ©es
```python
import sqlite3

# Stockage dans SQLite ou PostgreSQL
conn = sqlite3.connect('books.db')
df.to_sql('books', conn, if_exists='append', index=False)
```

3. Orchestration Airflow
```python
# DAG pour scraping quotidien
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG(
    'books_scraper',
    schedule_interval='@daily',
    # ... configuration
)
```

4. Monitoring et alertes
```python
# MÃ©triques de qualitÃ©
- Nombre de livres collectÃ©s par jour
- Taux d'erreur
- Temps d'exÃ©cution
- Alertes si site modifiÃ© (structure HTML changÃ©e)
```

5. API de donnÃ©es
```python
# Exposer les donnÃ©es via FastAPI
from fastapi import FastAPI

app = FastAPI()

@app.get("/books")
def get_books(min_price: float = 0, max_price: float = 100):
    # Retourner les livres filtrÃ©s
    pass
```

 ğŸ”’ ConsidÃ©rations lÃ©gales et Ã©thiques

 Bonnes pratiques de scraping

âœ… Ã€ faire :
- Consulter le `robots.txt` du site
- Respecter les rate limits (1-2 sec entre requÃªtes)
- Utiliser un User-Agent identifiable
- Ne pas surcharger les serveurs (heures creuses)
- Respecter les CGU du site

âŒ Ã€ Ã©viter :
- Scraping intensif (DDoS involontaire)
- Contournement de mesures anti-scraping
- Revente de donnÃ©es scrapÃ©es sans autorisation
- Scraping de donnÃ©es personnelles (RGPD)

 Note lÃ©gale
Ce projet est Ã  but Ã©ducatif uniquement. Le site `books.toscrape.com` est un site de dÃ©monstration crÃ©Ã© spÃ©cifiquement pour l'apprentissage du web scraping.

 ğŸ“š CompÃ©tences dÃ©veloppÃ©es

 Techniques
- Web scraping : BeautifulSoup, sÃ©lecteurs CSS, parsing HTML
- HTTP requests : Gestion de requÃªtes, headers, responses
- Data wrangling : Manipulation de listes, transformation en DataFrame
- Python : Boucles, conditions, gestion de strings

 MÃ©thodologiques
- Analyse de la structure HTML d'un site web
- Identification des sÃ©lecteurs CSS pertinents
- Gestion de donnÃ©es semi-structurÃ©es (HTML â†’ DataFrame)
- Debugging d'extraction de donnÃ©es

 Domaine mÃ©tier
- E-commerce : ComprÃ©hension des catalogues produits
- Data collection : Processus ETL (Extract, Transform, Load)
- Veille concurrentielle : Collecte automatisÃ©e de donnÃ©es marchÃ©

 ğŸ”§ Reproduction du projet

 PrÃ©requis

```bash
# Python 3.7+
python --version

# Installation des dÃ©pendances
pip install beautifulsoup4
pip install requests
pip install pandas
```

Ou via `requirements.txt` :
```txt
beautifulsoup4==4.12.2
requests==2.31.0
pandas==2.1.0
```

```bash
pip install -r requirements.txt
```

 Utilisation

```bash
# Lancer le scraper
python book_scraper.py
```

Output attendu :
```
     Title                          Price    Availability
0    A Light in the Attic          Â£51.77   In stock
1    Tipping the Velvet            Â£53.74   In stock
...
```

 Structure du projet

```
book-scraper/
â”œâ”€â”€ book_scraper.py          # Script principal
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ README.md               # Documentation
â”œâ”€â”€ data/                   # (futur) DonnÃ©es collectÃ©es
â”‚   â””â”€â”€ books_data.csv
â”œâ”€â”€ logs/                   # (futur) Logs d'exÃ©cution
â””â”€â”€ tests/                  # (futur) Tests unitaires
    â””â”€â”€ test_scraper.py
```

 ğŸ“ Cas d'usage rÃ©els

Ce type de scraper peut Ãªtre adaptÃ© pour :

1. E-commerce : Veille prix concurrence (Amazon, Cdiscount)
2. Immobilier : Collecte d'annonces (SeLoger, LeBonCoin)
3. Emploi : AgrÃ©gation d'offres d'emploi (LinkedIn, Indeed)
4. ActualitÃ©s : Monitoring de news (Le Monde, Les Ã‰chos)
5. RÃ©seaux sociaux : Analyse de tendances (via APIs officielles)

 ğŸ“Š Extensions possibles

 1. Analyse de donnÃ©es
```python
# Statistiques sur les prix
df['Price_Clean'] = df['Price'].str.replace('Â£', '').astype(float)
print(f"Prix moyen : Â£{df['Price_Clean'].mean():.2f}")
print(f"Prix mÃ©dian : Â£{df['Price_Clean'].median():.2f}")
```

 2. Visualisation
```python
import matplotlib.pyplot as plt

# Distribution des prix
df['Price_Clean'].hist(bins=20)
plt.title('Distribution des prix des livres')
plt.xlabel('Prix (Â£)')
plt.ylabel('Nombre de livres')
plt.show()
```

 3. Machine Learning
```python
# PrÃ©diction de prix basÃ©e sur titre/catÃ©gorie
from sklearn.ensemble import RandomForestRegressor
# ... feature engineering + model training
```

 ğŸ“– Contexte acadÃ©mique

Projet rÃ©alisÃ© dans le cadre : Master Data Engineer  
Cours : Web Scraping et Collecte de donnÃ©es  
DurÃ©e : 1 journÃ©e  
Objectif pÃ©dagogique : MaÃ®triser les fondamentaux du web scraping avec Python

 ğŸ”— Ressources complÃ©mentaires

- [Documentation BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Requests Library](https://requests.readthedocs.io/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Web Scraping Best Practices](https://www.scrapehero.com/web-scraping-best-practices/)

 ğŸ“§ Contact

Franck Ulrich BIPANDA  
ğŸ“§ bipanda.franck@icloud.com  
ğŸ”— [LinkedIn](https://linkedin.com/in/franck-bipanda-13392372)  
ğŸŒ [Portfolio](https://datascienceportfol.io/bipandaf)

â­ Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  le star !
